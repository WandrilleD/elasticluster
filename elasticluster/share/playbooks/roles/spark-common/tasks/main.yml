# spark-common/tasks/main.yml
---

- name: Install Spark packages (common)
  tags:
    - hadoop
    - spark
    - common
  package:
    name='{{item}}'
    state=present
  with_items:
    - spark-core # Lightning-Fast Cluster Computing
    - spark-datanucleus # DataNucleus libraries for Apache Spark
    - spark-extras # External/extra libraries for Apache Spark
    - spark-python # Python client for Spark


- name: Ensure Spark configuration directory exists
  tags:
    - hadoop
    - spark
  file:
    path='{{SPARK_CONF_DIR}}'
    state=directory
    
    
- name: Copy Spark/BigTop default configuration files
  tags:
    - hadoop
    - spark
  command:
    'rsync -ax --update --backup /etc/spark/conf.dist/ {{SPARK_CONF_DIR}}/'

  
- name: Activate Spark/ElastiCluster configuration
  tags:
    - hadoop
    - spark
  alternatives:
    name='spark-conf'
    link='/etc/spark/conf'
    path='{{SPARK_CONF_DIR}}'


- name: Deploy PySpark configuration files
  tags:
    - hadoop
    - spark
  copy:
    src='{{item}}'
    dest='/{{item}}'
  with_items:
    - etc/profile.d/pyspark.sh
